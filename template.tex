\documentclass[xcolor=table,slidestop,compress,mathserif]{beamer}
\beamertemplatenavigationsymbolsempty
%\usepackage[no-math]{fontspec}
%\usepackage{xeCJK}
%\setCJKmainfont[BoldFont={Adobe Heiti Std}]{Adobe Song Std}
%\punctstyle{kaiming}
\usepackage{amssymb}
\usepackage{media9}
\usepackage{multimedia}
% setup tikz 
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows}
\tikzstyle{arrow} = [thick,->,>=stealth]
\tikzstyle{roundRec} = [rectangle, rounded corners, minimum width=1.5cm, minimum height=0.8cm,text centered, draw=black]
\tikzstyle{roundRecS} = [rectangle, rounded corners, minimum width=1.5cm, minimum height=0.5cm,text centered, draw=black]
\tikzstyle{rec} = [rectangle, minimum width=1.5cm, minimum height=1cm,text centered, draw=black]
\DeclareGraphicsExtensions{.pdf,.eps,.png,.jpg,.jpeg}
\graphicspath{{./figure/}}

\setbeamertemplate{items}[circle]
\usetheme{Warsaw}
\setbeamertemplate{headline}{}
\defbeamertemplate*{footline}{shadow theme}
{%
  \leavevmode%
  \hbox{\begin{beamercolorbox}[wd=.5\paperwidth,ht=2.5ex,dp=1.125ex,leftskip=.3cm
      plus1fil,rightskip=.3cm]{author in head/foot}%
    \usebeamerfont{author in
      head/foot}\insertshortinstitute\hfill\insertshortauthor
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.5ex,dp=1.125ex,leftskip=.3cm,rightskip=.3cm
    plus1fil]{title in head/foot}%
    \usebeamerfont{title in
      head/foot}\hfill\insertframenumber\,/\,\inserttotalframenumber%
  \end{beamercolorbox}}%
  \vskip0pt%
}
\usepackage{graphicx}

\pgfdeclareimage[height=0.618cm]{logo}{figures/sjtulogoblue.png}
\logo{\pgfuseimage{logo}}

\def\hilite<#1>{%
  \temporal<#1>{\color{gray}}{\color{blue}}%
  {\color{blue!25}}}
% ------------------------------------------
\title{Audio Event Detection for \\ Automatic Scene Recognition}
\author{Xu Xun}
\institute[CS SJTU]{Department of Computer Science and Engineering \\ Shanghai
  Jiao Tong University}
\date{\today}
\begin{document}
\frame{\titlepage}

% show the outline 
\begin{frame}<beamer>[shrink=10]{Outline}
  \tableofcontents[sectionstyle=show,subsectionstyle=hide]
\end{frame}

% show outline when switch sections
\AtBeginSection[]{
  \begin{frame}<trans|beamer>[shrink=10]{Outline}
    \tableofcontents[sectionstyle=show/shaded,subsectionstyle=hide]
  \end{frame}
}

% ------------------------------------------

\section{Introduction}
\subsection{Problem Description}
\begin{frame}
  \frametitle{Problem Description}
	In this project, our problem is to recognize a scene where an audio is recorded. \\ 
	
	\sound[label=show1, inlinesound]{}{show.wav}
	\hyperlinkmovie[start=0s]{show1}{\beamerbutton{Play Sound}} \\

	\pause
	%waveform 
	\vspace{0.5cm}
	\begin{columns}[c]
		\begin{column}{0.5\textwidth}
			\includegraphics[scale=0.4]{./figure/office.eps}
		\end{column}
		\begin{column}{0.05\textwidth}
			\centering
			$\Rightarrow$
		\end{column}
		\begin{column}{0.45\textwidth}
			\em{office}
		\end{column}
	\end{columns}

\end{frame}
% ------------------------------------------
\begin{frame}
	\frametitle{Problem Description}
	\begin{itemize}
		\item{Scene\\} 
		An acoustic environment, like \textit{office}, \textit{bathroom}, etc. \\ 
		\item{Event\\} 
		A more short, primitive sound, like \textit{phone}, \textit{printer}, etc.
	\end{itemize}
\end{frame}
% ------------------------------------------
\subsection{Approach}
\begin{frame}
  \frametitle{Our approach}
	Our approach is to detect the audible events in a clip. \\
	Then infer the scene from the detected events. 	
		
	\vspace{0.5cm}
	\begin{columns}[c]
		\begin{column}{0.5\textwidth}
			\includegraphics[scale=0.4]{./figure/office.eps}
		\end{column}
		\begin{column}{0.05\textwidth}
			$\Rightarrow$
		\end{column}
		\begin{column}{0.15\textwidth}
			\textit{phone, printer}
		\end{column}
		\begin{column}{0.05\textwidth}
			$\Rightarrow$
		\end{column}
		\begin{column}{0.2\textwidth}
			\textit{office}
		\end{column}
	\end{columns}

\end{frame}
% ------------------------------------------
\begin{frame}
	\frametitle{Our Approach vs. Other Approaches}
	\begin{columns}[c]
		\begin{column}{0.5\textwidth}
			Our approach:\\ 
			\vspace{0.5cm}
			\resizebox{0.8\textwidth}{!}{
			\begin{tikzpicture}[node distance=1.5cm]
				\node (audio) [roundRec] {Audio}; 
				\node (event) [roundRec, below of=audio] {Event detection}; 
				\node (scene) [roundRec, below of=event] {Scene recognition}; 
				\draw [arrow] (audio) -- (event);
				\draw [arrow] (event) -- (scene); 
			\end{tikzpicture}
			}
		\end{column}
		\begin{column}{0.5\textwidth}
			Other approaches: \\ 
			\vspace{0.5cm}
			\resizebox{0.8\textwidth}{!}{
			\begin{tikzpicture}[node distance=1.5cm]
				\node (audio) [roundRec] {Audio}; 
				\node (scene) [roundRec, below of=event] {Scene recognition}; 
				\draw [arrow] (audio) -- (scene);
			\end{tikzpicture}
			}
		\end{column}
	\end{columns}

\end{frame}
% ------------------------------------------
\section{Audio Event Detection}
\subsection{Audible Event Taxonomy}
\begin{frame}
	\frametitle{Audible Event Taxonomy}	
	We labelled common audible events into 4 classes.\\
	There are 120 events in total. \\ 
	\vspace{0.5cm}
	\begin{tikzpicture}[node distance=1.5cm, every text node part/.style={align=center}]
		\node (event) [roundRec] {Audible Events}; 
		\node (human) [roundRec, below of=event, xshift=-4cm] {Human}; 
		\node (nonliving) [roundRec, below of=event, xshift=-1.25cm] {Non-Living}; 
		\node (nature) [roundRec, below of=event, xshift=1.25cm] {Nature}; 
		\node (other) [roundRec, below of=event, xshift=4cm] {Other Sound}; 
		\node (movement) [roundRec, below of=human, xshift=-1cm] {Movement}; 
		\node (nonmovement) [roundRec, below of=human, xshift=2cm] {Non-Movement}; 
		\node (animal) [roundRec, below of=nature] {Animal}; 
		\node (events1) [rec, below of=movement] {\textcolor{blue}{footstep}, \\ \textcolor{blue}{running}, \\ ...}; 
		\node (events2) [rec, below of=nonmovement] {\textcolor{blue}{cough}, \\ \textcolor{blue}{laughter}, \\ ...}; 
		\node (events3) [rec, below of=animal] {\textcolor{blue}{dog}, \\ \textcolor{blue}{cat}, \\ ...}; 
		\draw [arrow] (event) -- (human);
		\draw [arrow] (event) -- (nonliving);
		\draw [arrow] (event) -- (nature);
		\draw [arrow] (event) -- (other); 
		\draw [arrow] (human) -- (movement); 
		\draw [arrow] (human) -- (nonmovement); 
		\draw [arrow] (nature) -- (animal); 
		\draw [arrow] (movement) -- (events1); 
		\draw [arrow] (nonmovement) -- (events2); 
		\draw [arrow] (animal) -- (events3); 
	\end{tikzpicture}
\end{frame}
% ------------------------------------------
\subsection{Audio Data}
\begin{frame}
  \frametitle{Audio Data}	
	We download the audio data for events from \\ Sound Search Engines (SSEs). \\ 
	For example, when we query ``cough'' in SSE: \\ 
	\begin{figure}
	\includegraphics[scale=0.35]{figure/cough.png}
	\end{figure}
	We download clips from 1 second to 60 seconds. 
\end{frame}
% ------------------------------------------
\subsection{Preprocess and Feature Extraction}
\begin{frame}
	\frametitle{Preprocess and Feature Extraction}
	We first use Minimum Statistics to calculate the noise spectrum and subtract it from the input signal. \\ 
	Then we extract Mel-Frequency Cepstral Coefficients (MFCCs) from denoised signal.\\  
	\pause 
	% flowchart of mfcc 
	\begin{columns}[T]
	\begin{column}{0.5\textwidth}
	\begin{tikzpicture}[node distance=1cm]
		\node (audio) [roundRecS] {Denoised signal}; 
		\node (frame) [roundRecS, below of=audio] {Framing}; 
		\node (fft) [roundRecS, below of=frame] {Fast Fourier Transform (FFT)}; 
		\node (mel) [roundRecS, below of=fft] {Mel filtering}; 
		\node (envelope) [roundRecS, below of=mel] {Extract spectral envelope}; 
		\node (mfcc) [roundRecS, below of=envelope] {MFCCs}; 
		\draw [arrow] (audio) -- (frame);
		\draw [arrow] (frame) -- (fft); 
		\draw [arrow] (fft) -- (mel); 
		\draw [arrow] (mel) -- (envelope); 
		\draw [arrow] (envelope) -- (mfcc); 
	\end{tikzpicture}
	\end{column}	
	\pause
	\begin{column}{0.5\textwidth}
		\begin{figure}
			\includegraphics[scale=0.4]{figure/spectrum.eps}
			\caption{Audio in frequency domain}
		\end{figure}
	\end{column}
	\end{columns}
\end{frame}
% ------------------------------------------
\subsection{Event Model}
\begin{frame}
	\frametitle{Event Model}	
	We use features to train Gaussian Mixture Models (GMMs). \\ 
	The training is done by Expectation-Maximization (EM) algorithm. \\  
	\begin{figure}
		%\resizebox{0.8\totalheight}{!}{\input{figure/gmm.png}}
		\includegraphics[scale=0.6]{figure/gmm.pdf}
		\caption{A GMM with three components}
	\end{figure}
\end{frame}
% ------------------------------------------
\section{Scene Recognition}
%\subsection{Scene Extraction}
%\begin{frame}
%	\frametitle{Scene Extraction}	
%	We use the scripts for movies, plays and TV series to extract the scenes. \\ 
%	Below is a script example. 
%	We call it a \textit{context}, including a \textit{scene-switching sentence}, and its content. 
%	\begin{table}[h]
%		\begin{tabular}{|l|}
%		\hline
%		INT. LEONARD'S \textcolor{red}{BATHROOM} - Night \\ 
%		Leonard turns on the light, revealing a shower, toilet and sink.\\
%		He removes toiletries from the grocery bag and places them inside. \\ 
%		\hline
%		\end{tabular}
%	\end{table}
%\end{frame}
%% ------------------------------------------
%\begin{frame}
%	\frametitle{Scene Extraction}
%	We use Natural Language Process (NLP) tools to process the \textit{scene-switching sentence}, and eliminate the following type of words: \\ 
%	\begin{itemize}
%		\item{Person names} 
%		\item{Time indicator}
%		\item{Location names}
%		\item{Adjective, determiner, number, ...}
%	\end{itemize}
%
%	\begin{table}[htb]
%	\centering
%	\caption{Top 10 Occurred Scenes}
%	\resizebox{0.3\textwidth}{!}{
%		\begin{tabular}{ll}
%		\hline
%		Scene & Occurrence \\
%		\hline
%		house & 3537 \\ 
%		office & 3259 \\ 
%		apartment & 2919 \\ 
%		room & 2580 \\ 
%		bedroom & 2257 \\ 
%		car & 1699 \\ 
%		street & 1622 \\ 
%		kitchen & 1431 \\ 
%		living room & 1374 \\ 
%		tardis & 1259 \\ 
%		\hline
%		\end{tabular}
%	}
%	\end{table}
%\end{frame}
% ------------------------------------------
\subsection{Scene-Event Relation Mining}
\begin{frame}
	\frametitle{Scene-Event Relation Mining}
	To get the relation between scenes and audible events, \\ 
	we match the context in a script with our predefined audible events. \\ 
	\begin{table}[h]
		\begin{tabular}{|l|}
		\hline
		INT. LEONARD'S \textcolor{red}{BATHROOM} - Night \\ 
		Leonard turns on the light, revealing a \textcolor{blue}{shower}, \textcolor{blue}{toilet} and \textcolor{blue}{sink}.\\
		He removes toiletries from the grocery bag and places them inside. \\ 
		\hline
		\end{tabular}
	\end{table}
\end{frame}
% ------------------------------------------
%\begin{frame}
%	\frametitle{Scene-Event Relation Mining}
%	Based on the idea of Term-Frequency-Inverse Document Frequency (TFIDF), we calculate two scores of an event $e$, to a scene $s$. 
%	\begin{enumerate}
%		\item{$TF=log(1+f(e,s))$}\\
%		$f(e,s)$ is the number of contexts $e$ appears in all contexts under scene $s$. 
%		\item{$IDF=1+log(\frac{N}{N_e})$} \\ 
%		$N$ is the number of scenes. 
%		$N_e$ is the number of scenes in which event $e$ appears. 
%	\end{enumerate}
%	These two scores are then multiplied, and used as the importance of an event to a scene. 		
%	\begin{equation}
%		TFIDF = TF \times IDF 
%	\end{equation}	
%\end{frame}
% ------------------------------------------
\begin{frame}
	\frametitle{Scene-Event Relation Mining}
	\begin{table}[htb!]
	\caption{An example of scene-event map}
	\resizebox{0.8\textwidth}{!}{
	\begin{tabular}{ll}
	\hline
	Scene     & Top 10 events ranked by TF-IDf\\
	\hline
	bathroom & running+water, toilet, faucet, toothbrush, \\ 
		& shower, drawer, drain, talk, paper, bowl \\ 
	beach & seagull, sand, boat, talk, wave, sea, \\ 
		& car, laughter, drink, wood, running \\ 
	forest & tree, wood, dirt, talk, running, bird,\\
		&  river, car, leaf, grass, wind\\ 
	kitchen & drawer, cutlery, microwave, dish, kettle, \\ 
		& talk, bowl, phone, toaster, running+water\\ 
	street & car, truck, subway, talk, traffic, \\ 
		& engine, siren, phone, running, laughter \\ 
	\hline
	\end{tabular}
	}
	\end{table}
\end{frame}
% ------------------------------------------
\subsection{Audio Segmentation}
\begin{frame}
	\frametitle{Audio Segmentation}	
	Scene-Event map is used when we have detected the events. \\ 
	We need to cut testing clips into segments for event detection. \\ 
	\begin{figure}
	\includegraphics[scale=0.5]{figure/tune_part.eps}
	\caption{A example audio clip}
	\end{figure}
	%\begin{enumerate}
	%	\item{Frame Energy} \\ 
	%	The averaged energy of a frame, calculated as: 
	%	\begin{equation}
	%	E_i = \frac{\sum\limits_{n=1}^N(x_i(n))}{N}
	%	\end{equation}
	%	\item{Spectral Centroid} \\ 
	%	The ``center'' of frequency, calculated as: 
	%	\begin{equation}
	%	C_i = \frac{\sum\limits_{k=1}^Nk\times Amp(k)}{\sum\limits_{k=1}^NAmp(k)}
	%	\end{equation}
	%\end{enumerate}
\end{frame}
% ------------------------------------------
\begin{frame}
	\frametitle{Audio Segmentation}
	We use frame energy and frequency to filter out silence and noise. \\ 	
	\vspace{-0.25cm}
	\begin{figure}[htb]
	\centering
	\includegraphics[scale=0.5]{figure/segment.pdf}
	\caption{A segmentation example}
	\label{fig:segment}
	\end{figure}
\end{frame}
% ------------------------------------------
\subsection{Scene Inference}
\begin{frame}
	\frametitle{Scene Inference}	
	For each segment, we evaluate it with our trained GMMs. \\ 
	We choose the top three detected events for scene voting. \\ 
	\begin{tikzpicture}[node distance=1.5cm, every text node part/.style={align=center}]
		\centering
		\node (audio) [roundRec] {Test audio}; 
		\node (seg1) [roundRec, below of=audio, xshift=-2cm] {Seg 1}; 
		\node (seg2) [roundRec, below of=audio] {Seg 2}; 
		\node (segn) [roundRec, below of=audio, xshift=2cm] {Seg n}; 
		\node (event1) [roundRec, below of=seg1] {Event 1, \\ Event 2, \\ Event 3}; 
		\node (event2) [roundRec, below of=seg2] {Event 1, \\ Event 2, \\ Event 3}; 
		\node (eventn) [roundRec, below of=segn] {Event 1, \\ Event 2, \\ Event 3}; 
		\node (map) [roundRec, below of=event2] {Scene-event map}; 	
		\node (scene) [roundRec, below of=map] {Recognized scene};
		\draw [arrow] (audio) -- (seg1);
		\draw [arrow] (audio) -- (seg2);
		\draw [arrow] (audio) -- (segn);
		\draw [arrow] (seg1) -- (event1); 
		\draw [arrow] (seg2) -- (event2); 
		\draw [arrow] (segn) -- (eventn); 
		\draw [arrow] (event1) -- (map); 
		\draw [arrow] (event2) -- (map); 
		\draw [arrow] (eventn) -- (map); 
		\draw [arrow] (map) -- (scene); 
	\end{tikzpicture}
\end{frame}
% ------------------------------------------
\section{Evaluation}
\subsection{Event Detection Evaluation}
\begin{frame}
	\frametitle{Component Number Evaluation}
	Gaussian Mixture Model distribution: \\ 
	\begin{equation}
	P(\mathbf{x}|\mathbf{\pi},\mathbf{\mu},\Sigma) = \sum_{k = 1}^{\textcolor{red}{M}} \pi_k
	\mathcal{N}(\mathbf{x}|\mathbf{\mu}_k, \Sigma_k),
	\end{equation} 

	\begin{figure}[htb!]
	\centering
	\resizebox{0.75\totalheight}{!}{\input{figure/component.tex}}
	\caption{F-measure for different component number}
	\label{fig:component}
	\end{figure}
\end{frame}
% ------------------------------------------
\begin{frame}
	\frametitle{Componnent Number Evaluation}
	After comparing F-measure and running time, we choose 18 as our component number. 
	\begin{figure}[htb!]
	\centering
	\resizebox{0.8\totalheight}{!}{\input{figure/componentTime.tex}}
	\caption{Running time for different component number}
	\label{fig:component}
	\end{figure}
\end{frame}
% ------------------------------------------
\subsection{Scene Recognition Evaluation}
\begin{frame}
	\frametitle{Scene Recognition Evaluation}
	In scene recognition, we choose 10 scenes, each scene has 10 clips. \\ 	
	Accuracy for other 4 systems are calculated using 5-fold cross validation. 
	Our system achieve an accuracy of 57\%.
	\begin{figure}[htb!]
	\centering
	\resizebox{0.8\totalheight}{!}{\input{figure/sceneeval.tex}}
	\caption{Recognition accuracy for 10 audio scenes}
	\label{fig:sceneeval}
	\end{figure}
\end{frame}
% ------------------------------------------
\begin{frame}
	\frametitle{Scene Recognition Evaluation}
	Detailed result of our system with the best system \textit{GSR}. \\ 
	\begin{figure}[htb!]
	\centering
	\resizebox{0.9\totalheight}{!}{\input{figure/ourgsr.tex}}
	\caption{10-scenes classification}
	\label{fig:sceneeval}
	\end{figure}
\end{frame}
% ------------------------------------------
\section{Conclusion}
\begin{frame}
	\frametitle{Conclusion}
	\begin{itemize}
		\item{We build a scene recognition system from event detection.\\}
		\item{Our system has the advantange of expanding to many scenes without new scene data.\\}
		\item{We could outperform existing approaches in scenes where audible events are easy to capture.\\}
	\end{itemize}
\end{frame}
% ------------------------------------------
\section{Demo}
\begin{frame}
	\frametitle{Demo}
	Live demo for our system. 
\end{frame}
% ------------------------------------------
%\setbeamertemplate{background canvas}[vertical shading][bottom=white,top=structure.fg!25]
\begin{frame}
  \begin{center}
    {\huge \emph{{Thank  ~you!
          \\   \vspace{1cm} Any Question?}}}
  \end{center}
\end{frame}
\end{document}
